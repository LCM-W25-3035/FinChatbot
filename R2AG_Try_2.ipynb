{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32af4069",
   "metadata": {},
   "source": [
    "### ChatGPT first prompt :\n",
    "\n",
    "\"Identify the key areas of improvements in the code you previously gave for R2AG and rewrite them to improve the accuracy of the model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9af191",
   "metadata": {},
   "source": [
    "### ChatGPT last prompt :\n",
    "\n",
    "\" I'm getting the following error :\n",
    "TypeError: sequence item 0: expected str instance, dict found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c92f5c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sehaj\\OneDrive\\Desktop\\Capstone\\FinChatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "import sklearn\n",
    "from sklearn.metrics import f1_score\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a28c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing OpenAI client\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcc6b03",
   "metadata": {},
   "source": [
    "To evaluate this model, we will use the MuSiQue dataset using pandas which consists of questions with their ground truths/answers for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e71a374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "splits = {'train': 'musique_ans_v1.0_train.jsonl', 'validation': 'musique_ans_v1.0_dev.jsonl'}\n",
    "df = pd.read_json(\"hf://datasets/dgslibisey/MuSiQue/\" + splits[\"train\"], lines=True)\n",
    "df = df[[\"question\", \"answer\", \"paragraphs\"]].dropna()\n",
    "df = df.head(10)\n",
    "df[\"answer\"] = df[\"answer\"].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c45850b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# R2Former definition\n",
    "class R2Former(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(R2Former, self).__init__()\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim=input_dim, num_heads=8, batch_first=True)\n",
    "        self.linear = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.self_attention(x, x, x)\n",
    "        return self.linear(attn_output)\n",
    "\n",
    "# Initializing R2Former with correct input_dim to match embedding dim\n",
    "r2former = R2Former(input_dim=384, hidden_dim=384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b071a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever class\n",
    "class Retriever:\n",
    "    def __init__(self, documents, model):\n",
    "        self.documents = documents\n",
    "        self.model = model\n",
    "        self.doc_embeddings = model.encode(documents, convert_to_tensor=True)\n",
    "\n",
    "    def retrieve(self, query, top_k=5):\n",
    "        query_embedding = self.model.encode([query], convert_to_tensor=True)\n",
    "        cos_scores = torch.nn.functional.cosine_similarity(query_embedding, self.doc_embeddings)\n",
    "        top_results = torch.topk(cos_scores, k=top_k)\n",
    "        return [self.documents[idx] for idx in top_results.indices.tolist()]\n",
    "\n",
    "# Preparing corpus for retrieval\n",
    "corpus = df[\"paragraphs\"].tolist()\n",
    "retriever = Retriever(corpus, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51b11bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-ranking using R2Former\n",
    "def transform_retrievals(retriever, query, r2former, top_k=5):\n",
    "    retrieved_docs = retriever.retrieve(query, top_k=top_k)\n",
    "    embeddings = retriever.model.encode(retrieved_docs, convert_to_tensor=True)\n",
    "    embeddings = embeddings.unsqueeze(0)  # Adding batch dim\n",
    "    refined_embeddings = r2former(embeddings).squeeze(0)\n",
    "    query_embedding = retriever.model.encode([query], convert_to_tensor=True).squeeze(0)\n",
    "    sim_scores = torch.nn.functional.cosine_similarity(refined_embeddings, query_embedding.unsqueeze(0), dim=1)\n",
    "    top_indices = torch.topk(sim_scores, k=3).indices.tolist()\n",
    "\n",
    "    top_docs = []\n",
    "    for i in top_indices:\n",
    "        doc = retrieved_docs[i]\n",
    "        \n",
    "        # If doc is a dictionary, trying to get the text key\n",
    "        if isinstance(doc, dict):\n",
    "            text = doc.get('text', '')  \n",
    "            top_docs.append(text)\n",
    "        \n",
    "        # If doc is a list, joining all its elements into a single string\n",
    "        elif isinstance(doc, list):\n",
    "            # Assuming the list contains text data\n",
    "            top_docs.append(\" \".join(str(item) for item in doc))  # Converting each item to string and join\n",
    "        \n",
    "        # If doc is a plain string, appending it directly\n",
    "        elif isinstance(doc, str):\n",
    "            top_docs.append(doc)\n",
    "\n",
    "    return refined_embeddings, top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62c9604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response generation\n",
    "def generate_response(context, query):\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant. You are given paragraphs as context. \n",
    "Only use the information present in these paragraphs to answer the question. \n",
    "Do not make up any information, and do not use external knowledge.\n",
    "Give only one or two word answer, nothing else.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer (only using the above context):\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c175e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Question: Why is exercise important?\n",
      "Generated Answer: Exercise is important because it helps to improve physical health, boosts mental well-being, and supports the immune system. It can prevent chronic diseases like heart disease, diabetes, and obesity. Additionally, exercise improves sleep, reduces stress, and boosts overall mood.\n"
     ]
    }
   ],
   "source": [
    "# Sample question and context for testing\n",
    "sample_context = \"\"\"\n",
    "Exercise is important because it helps to improve physical health, boosts mental well-being, and supports the immune system. Regular physical activity can prevent chronic diseases like heart disease, diabetes, and obesity. Additionally, it improves sleep, reduces stress, and boosts overall mood.\n",
    "\"\"\"\n",
    "sample_question = \"Why is exercise important?\"\n",
    "\n",
    "# Generating response using the sample context and question\n",
    "generated_answer = generate_response(sample_context, sample_question)\n",
    "\n",
    "# The result :\n",
    "print(f\"Sample Question: {sample_question}\")\n",
    "print(f\"Generated Answer: {generated_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa03359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE and F1 score evaluation\n",
    "def rouge_score_evaluation(predicted_answer, ground_truth):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "    scores = scorer.score(ground_truth, predicted_answer)\n",
    "    return scores[\"rougeL\"].fmeasure\n",
    "\n",
    "def f1_score_evaluation(predicted_answer, ground_truth):\n",
    "    pred_tokens = predicted_answer.lower().split()\n",
    "    true_tokens = ground_truth.lower().split()\n",
    "    common = set(pred_tokens) & set(true_tokens)\n",
    "    if not pred_tokens or not true_tokens:\n",
    "        return 0, 0\n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(true_tokens)\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "    return round(f1, 4), 1 if f1 > 0.5 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d647b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: When was the institute that owned The Collegian founded?\n",
      "Generated Answer: 1960\n",
      "Ground Truth: 1960\n",
      "ROUGE-L: 1.0\n",
      "F1 Score: 1.0\n",
      "\n",
      "Question: What year saw the creation of the region where the county of Hertfordshire is located?\n",
      "Generated Answer: 1994\n",
      "Ground Truth: 1994\n",
      "ROUGE-L: 1.0\n",
      "F1 Score: 1.0\n",
      "\n",
      "Question: When was the abolishment of the studio that distributed The Game?\n",
      "Generated Answer: Not mentioned\n",
      "Ground Truth: 1999\n",
      "ROUGE-L: 0.0\n",
      "F1 Score: 0\n",
      "\n",
      "Question: When was the publisher of Crux launched?\n",
      "Generated Answer: Not mentioned\n",
      "Ground Truth: 1998\n",
      "ROUGE-L: 0.0\n",
      "F1 Score: 0\n",
      "\n",
      "Question: Jan Å indel's was born in what country?\n",
      "Generated Answer: Not mentioned\n",
      "Ground Truth: Czech Republic\n",
      "ROUGE-L: 0.0\n",
      "F1 Score: 0\n",
      "\n",
      "Question: What city is the person who broadened the doctrine of philosophy of language from?\n",
      "Generated Answer: Copenhagen\n",
      "Ground Truth: Copenhagen\n",
      "ROUGE-L: 1.0\n",
      "F1 Score: 1.0\n",
      "\n",
      "Question: When was the baseball team winning the world series in 2015 baseball created?\n",
      "Generated Answer: Not mentioned\n",
      "Ground Truth: 1969\n",
      "ROUGE-L: 0.0\n",
      "F1 Score: 0\n",
      "\n",
      "Question: Where did the Baldevins bryllup director die?\n",
      "Generated Answer: Not mentioned\n",
      "Ground Truth: Copenhagen\n",
      "ROUGE-L: 0.0\n",
      "F1 Score: 0\n",
      "\n",
      "Question: Who was thee first president of the association that wrote the code of ethics for psychology?\n",
      "Generated Answer: G. Stanley Hall\n",
      "Ground Truth: G. Stanley Hall\n",
      "ROUGE-L: 1.0\n",
      "F1 Score: 1.0\n",
      "\n",
      "Question: Which major Russian city borders the body of water in which Saaremaa is located?\n",
      "Generated Answer: Saint Petersburg\n",
      "Ground Truth: Saint Petersburg\n",
      "ROUGE-L: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluation loop\n",
    "rouge_scores = []\n",
    "f1_preds = []\n",
    "f1_trues = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    query = row[\"question\"]\n",
    "    ground_truth = row[\"answer\"]\n",
    "    refined_embeddings, top_docs = transform_retrievals(retriever, query, r2former)\n",
    "    context = \"\\n\".join(top_docs)\n",
    "    generated_answer = generate_response(context, query)\n",
    "    rouge = rouge_score_evaluation(generated_answer, ground_truth)\n",
    "    f1_val, f1_binary = f1_score_evaluation(generated_answer, ground_truth)\n",
    "\n",
    "    rouge_scores.append(rouge)\n",
    "    f1_preds.append(f1_binary)\n",
    "    f1_trues.append(1)\n",
    "\n",
    "    print(f\"\\nQuestion: {query}\\nGenerated Answer: {generated_answer}\\nGround Truth: {ground_truth}\\nROUGE-L: {rouge}\\nF1 Score: {f1_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7df2278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average ROUGE-L Score: 0.5\n",
      "Overall F1 Score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation\n",
    "average_rouge = sum(rouge_scores) / len(rouge_scores)\n",
    "overall_f1 = f1_score(f1_trues, f1_preds)\n",
    "print(f\"\\nAverage ROUGE-L Score: {average_rouge}\")\n",
    "print(f\"Overall F1 Score: {overall_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8a7096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
