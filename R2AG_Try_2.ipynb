{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32af4069",
   "metadata": {},
   "source": [
    "### ChatGPT first prompt :\n",
    "\n",
    "\"Identify the key areas of improvements in the code you previously gave for R2AG and rewrite them to improve the accuracy of the model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9af191",
   "metadata": {},
   "source": [
    "### ChatGPT last prompt :\n",
    "\n",
    "\" I'm getting the following error :\n",
    "TypeError: sequence item 0: expected str instance, dict found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c92f5c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sehaj\\OneDrive\\Desktop\\Capstone\\FinChatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "import sklearn\n",
    "from sklearn.metrics import f1_score\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a28c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing OpenAI client\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcc6b03",
   "metadata": {},
   "source": [
    "To evaluate this model, we will use the MuSiQue dataset using pandas which consists of questions with their ground truths/answers for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e71a374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "splits = {'train': 'musique_ans_v1.0_train.jsonl', 'validation': 'musique_ans_v1.0_dev.jsonl'}\n",
    "df = pd.read_json(\"hf://datasets/dgslibisey/MuSiQue/\" + splits[\"train\"], lines=True)\n",
    "df = df[[\"question\", \"answer\", \"paragraphs\"]].dropna()\n",
    "df = df.head(10)\n",
    "df[\"answer\"] = df[\"answer\"].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c45850b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# R2Former definition\n",
    "class R2Former(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(R2Former, self).__init__()\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim=input_dim, num_heads=8, batch_first=True)\n",
    "        self.linear = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.self_attention(x, x, x)\n",
    "        return self.linear(attn_output)\n",
    "\n",
    "# Initializing R2Former with correct input_dim to match embedding dim\n",
    "r2former = R2Former(input_dim=384, hidden_dim=384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b071a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever class\n",
    "class Retriever:\n",
    "    def __init__(self, documents, model):\n",
    "        self.documents = documents\n",
    "        self.model = model\n",
    "        self.doc_embeddings = model.encode(documents, convert_to_tensor=True)\n",
    "\n",
    "    def retrieve(self, query, top_k=5):\n",
    "        query_embedding = self.model.encode([query], convert_to_tensor=True)\n",
    "        cos_scores = torch.nn.functional.cosine_similarity(query_embedding, self.doc_embeddings)\n",
    "        top_results = torch.topk(cos_scores, k=top_k)\n",
    "        return [self.documents[idx] for idx in top_results.indices.tolist()]\n",
    "\n",
    "# Preparing corpus for retrieval\n",
    "corpus = df[\"paragraphs\"].tolist()\n",
    "retriever = Retriever(corpus, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51b11bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-ranking using R2Former\n",
    "def transform_retrievals(retriever, query, r2former, top_k=5):\n",
    "    retrieved_docs = retriever.retrieve(query, top_k=top_k)\n",
    "    embeddings = retriever.model.encode(retrieved_docs, convert_to_tensor=True)\n",
    "    embeddings = embeddings.unsqueeze(0)  # Adding batch dim\n",
    "    refined_embeddings = r2former(embeddings).squeeze(0)\n",
    "    query_embedding = retriever.model.encode([query], convert_to_tensor=True).squeeze(0)\n",
    "    sim_scores = torch.nn.functional.cosine_similarity(refined_embeddings, query_embedding.unsqueeze(0), dim=1)\n",
    "    top_indices = torch.topk(sim_scores, k=3).indices.tolist()\n",
    "\n",
    "    top_docs = []\n",
    "    for i in top_indices:\n",
    "        doc = retrieved_docs[i]\n",
    "        \n",
    "        # If doc is a dictionary, trying to get the text key\n",
    "        if isinstance(doc, dict):\n",
    "            text = doc.get('text', '')  \n",
    "            top_docs.append(text)\n",
    "        \n",
    "        # If doc is a list, joining all its elements into a single string\n",
    "        elif isinstance(doc, list):\n",
    "            # Assuming the list contains text data\n",
    "            top_docs.append(\" \".join(str(item) for item in doc))  # Converting each item to string and join\n",
    "        \n",
    "        # If doc is a plain string, appending it directly\n",
    "        elif isinstance(doc, str):\n",
    "            top_docs.append(doc)\n",
    "\n",
    "    return refined_embeddings, top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62c9604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response generation\n",
    "def generate_response(context, query):\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant. You are given paragraphs as context. \n",
    "Only use the information present in these paragraphs to answer the question. \n",
    "Do not make up any information, and do not use external knowledge.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer (only using the above context):\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c175e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Question: Why is exercise important?\n",
      "Generated Answer: Exercise is important because it helps to improve physical health, boosts mental well-being, and supports the immune system. It can prevent chronic diseases like heart disease, diabetes, and obesity. Additionally, exercise improves sleep, reduces stress, and boosts overall mood.\n"
     ]
    }
   ],
   "source": [
    "# Sample question and context for testing\n",
    "sample_context = \"\"\"\n",
    "Exercise is important because it helps to improve physical health, boosts mental well-being, and supports the immune system. Regular physical activity can prevent chronic diseases like heart disease, diabetes, and obesity. Additionally, it improves sleep, reduces stress, and boosts overall mood.\n",
    "\"\"\"\n",
    "sample_question = \"Why is exercise important?\"\n",
    "\n",
    "# Generating response using the sample context and question\n",
    "generated_answer = generate_response(sample_context, sample_question)\n",
    "\n",
    "# The result :\n",
    "print(f\"Sample Question: {sample_question}\")\n",
    "print(f\"Generated Answer: {generated_answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
