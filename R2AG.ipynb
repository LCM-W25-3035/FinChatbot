{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT first prompt :\n",
    "- \"i am building a RAG model using gpt-4o-mini as my generator. I want you to give me steps with code on how to incorporate R2AG into my RAG model. \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT last prompt :\n",
    "- \"i am getting the following error here : \n",
    "TypeError: 'ChatCompletionMessage' object is not subscriptable\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementaton :\n",
    "\n",
    "First, run the following command in the terminal to install langchain, transformers, faiss-cpu, and sentence-transformers for retrieval and embedding operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain transformers faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sehaj\\OneDrive\\Desktop\\Capstone\\FinChatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import faiss\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentRetriever:\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None  # FAISS Index\n",
    "\n",
    "    def build_index(self, documents):\n",
    "        embeddings = self.model.encode(documents, convert_to_numpy=True)\n",
    "        self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        self.index.add(embeddings)\n",
    "        self.docs = documents  # Storing original documents\n",
    "\n",
    "    def retrieve(self, query, top_k=3):\n",
    "        query_embedding = self.model.encode([query], convert_to_numpy=True)\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        return [self.docs[i] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R2Former(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(R2Former, self).__init__()\n",
    "        self.projection = torch.nn.Linear(input_dim, hidden_dim)  # Projection layer\n",
    "        self.self_attention = torch.nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=8)\n",
    "\n",
    "    def forward(self, retrieval_embeddings):\n",
    "        projected_embeddings = self.projection(retrieval_embeddings)\n",
    "        attn_output, _ = self.self_attention(projected_embeddings, projected_embeddings, projected_embeddings)\n",
    "        return attn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_retrievals(retriever, query, r2former):\n",
    "    retrieved_docs = retriever.retrieve(query)\n",
    "    embeddings = retriever.model.encode(retrieved_docs, convert_to_tensor=True)\n",
    "    embeddings = embeddings.unsqueeze(0)  # Reshaping for attention\n",
    "    refined_embeddings = r2former(embeddings)\n",
    "    return refined_embeddings, retrieved_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def generate_response(query, refined_embeddings, retrieved_docs):\n",
    "    retrieval_context = \" \".join(retrieved_docs)\n",
    "    r2ag_prompt = f\"Context: {retrieval_context}\\n\\nQuery: {query}\\nAnswer:\"\n",
    "    \n",
    "    client = OpenAI() \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are an assistant.\"},\n",
    "                  {\"role\": \"user\", \"content\": r2ag_prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2AG typically refers to \"Rational and Robust Active Grid,\" which is a system or framework aimed at optimizing resource allocation and grid management in various contexts, such as electricity distribution or data processing. However, without specific context from the documents you mentioned (Document 1, Document 2, Document 3), I can't provide a more detailed or tailored answer. If those documents contain specific information regarding R2AG, I would need to reference them to give a more accurate response.\n"
     ]
    }
   ],
   "source": [
    "retriever = DocumentRetriever()\n",
    "retriever.build_index([\"Document 1 text...\", \"Document 2 text...\", \"Document 3 text...\"])  # Loading documents\n",
    "\n",
    "r2former = R2Former(input_dim=384, hidden_dim=768)  # Matching dimensions of LLM\n",
    "\n",
    "query = \"What is R2AG?\"\n",
    "refined_embeddings, retrieved_docs = transform_retrievals(retriever, query, r2former)\n",
    "\n",
    "response = generate_response(query, refined_embeddings, retrieved_docs)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R2AG Evaluation :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate this model, we will use the MuSiQue dataset using pandas which consists of questions with their ground truths/answers for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'musique_ans_v1.0_train.jsonl', 'validation': 'musique_ans_v1.0_dev.jsonl'}\n",
    "df = pd.read_json(\"hf://datasets/dgslibisey/MuSiQue/\" + splits[\"train\"], lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, we assume \"question\" and \"answer\" fields are strings\n",
    "df = df[[\"question\", \"answer\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
