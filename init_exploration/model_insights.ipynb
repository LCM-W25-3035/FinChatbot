{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abaa894a",
   "metadata": {},
   "source": [
    "### LangTrace AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c92d8461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5fcb981",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "\n",
    "LANGTRACE_API_KEY = os.getenv(\"LANGTRACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8fc269d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mInitializing Langtrace SDK..\u001b[39m\n",
      "\u001b[37m‚≠ê Leave our github a star to stay on top of our updates - https://github.com/Scale3-Labs/langtrace\u001b[39m\n",
      "Skipping openai due to error while instrumenting: No module named 'openai.resources.responses'\n",
      "\u001b[34mExporting spans to Langtrace cloud..\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rahul\\Desktop\\Capstone Project\\FinChatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langtrace_python_sdk import langtrace\n",
    "\n",
    "langtrace.init(api_key = LANGTRACE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec1964d",
   "metadata": {},
   "source": [
    "### MVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83c97e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a1c7529",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../artifacts/summaries/table_summaries.pkl\", \"rb\") as table_file:\n",
    "    table_summaries = pickle.load(table_file)\n",
    "with open(\"../artifacts/summaries/text_summaries.pkl\", \"rb\") as text_file:\n",
    "    text_summaries = pickle.load(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c6d3139",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../artifacts/original/table_original.pkl\", \"rb\") as table_file:\n",
    "    table = pickle.load(table_file)\n",
    "with open(\"../artifacts/original/text_original.pkl\", \"rb\") as text_file:\n",
    "    text = pickle.load(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5316373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_vector_retriever(vectorstore, text_summaries, texts, table_summaries, tables):\n",
    "\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"fintech-rag\"\n",
    "    \n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore = vectorstore,\n",
    "        docstore = store,\n",
    "        id_key = id_key,\n",
    "    )\n",
    "    \n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "\n",
    "        summary_docs = [\n",
    "            Document(page_content = str(s), metadata = {id_key: doc_ids[i]}) \n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "    \n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    \n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    \n",
    "    return retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02c9cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"../Database\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58d1210d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\AppData\\Local\\Temp\\ipykernel_2308\\4293712050.py:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorestore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "vectorestore = Chroma(\n",
    "    collection_name = \"rag-model\",\n",
    "    embedding_function = OpenAIEmbeddings(),\n",
    "    persist_directory = persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43195bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = create_multi_vector_retriever(\n",
    "    vectorstore = vectorestore,\n",
    "    table_summaries = table_summaries,\n",
    "    tables = table,\n",
    "    text_summaries = text_summaries,\n",
    "    texts = text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "815adf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\AppData\\Local\\Temp\\ipykernel_2308\\4104841141.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorestore.persist()\n"
     ]
    }
   ],
   "source": [
    "vectorestore.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33be978d",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0febbdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8628532e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\AppData\\Local\\Temp\\ipykernel_18936\\3665189655.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages = True)\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1ed2e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Conversation History:\n",
    "    {history}\n",
    "\n",
    "    Document Context:\n",
    "    {context}\n",
    "\n",
    "    Current Question:\n",
    "    {question}\n",
    "\n",
    "    Generate a response that thoughtfully integrates both the conversation history \\\n",
    "    and the provided document context, emphasizing finance-specific details when applicable. \\\n",
    "    The answer should be concise and clear, ranging between 10 and 200 words based on the \\\n",
    "    complexity of the question. Only include information directly supported by the given context.\n",
    "\n",
    "    Important Instructions:\n",
    "    - If the current question relates solely to previous inquiries or lacks new context, first verify the conversation history(especially the last question asked). If it is not connected to any prior question, reply with: \"The pdf doesn't contain context regarding the question.\"\n",
    "    - For finance-related inquiries, incorporate appropriate financial terminology and domain expertise.\n",
    "    - Do not add any external details not present in the document context.\n",
    "    - Highlight all critical numbers and percentages in **bold**.\n",
    "\n",
    "    Policies:\n",
    "    - NEVER infer relationships between financial concepts.\n",
    "    - PRESERVE the original context's numerical precision.\n",
    "    - Strictly adhere to the provided document context (mvr); avoid introducing external details.\n",
    "    - Use clear, user-friendly language throughout the response.\n",
    "    - Ensure all information is derived solely from the given context and conversation history.\n",
    "    - Maintain accuracy and clarity without unnecessary elaboration.\n",
    "    - Use bullet points where necessary.\n",
    "\n",
    "    IF THE OUTPUT CANNOT BE GENERATED FROM THE CONTEXT, JUST REPLY WITH - \"The pdf doesn't contain context regarding the question.\"\n",
    "    \"\"\"\n",
    "        )\n",
    "\n",
    "\n",
    "# LLM\n",
    "model = ChatOpenAI(temperature = 0, model = \"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6da40061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"history\": lambda x: memory.load_memory_variables({})[\"history\"],\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07cc6915",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"What was the revenue in Q1 2023?\"\n",
    "output = chain.invoke({\"question\": user_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a0302f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The pdf doesn't contain context regarding the question.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
