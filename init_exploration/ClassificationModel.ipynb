{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Prompt to chatgpt: I need a best model to classify whether the question is text or arithmetic\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import pandas as pd  #importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'answer', 'answer_type', 'derivation'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../Data/questions_table.csv\")  #load the dataset\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['multi-span', 'span', 'arithmetic', 'count'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"answer_type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data[data[\"answer_type\"].isin([\"span\", \"arithmetic\"])]   #get the answer_type as span or arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11265"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_type</th>\n",
       "      <th>derivation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How much is the 2019 rate of inflation?</td>\n",
       "      <td>['2.9']</td>\n",
       "      <td>span</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How much is the 2018 rate of inflation?</td>\n",
       "      <td>['2.9']</td>\n",
       "      <td>span</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the 2019 average rate of inflation?</td>\n",
       "      <td>2.9</td>\n",
       "      <td>arithmetic</td>\n",
       "      <td>(2.9+2.9)/2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the 2019 average rate of increase in s...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>arithmetic</td>\n",
       "      <td>(2.7+2.7)/2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is the difference between 2019 average ra...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>arithmetic</td>\n",
       "      <td>[(2.9+2.9)/2] - [(2.7+2.7)/2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question   answer answer_type  \\\n",
       "1            How much is the 2019 rate of inflation?  ['2.9']        span   \n",
       "2            How much is the 2018 rate of inflation?  ['2.9']        span   \n",
       "3        What is the 2019 average rate of inflation?      2.9  arithmetic   \n",
       "4  What is the 2019 average rate of increase in s...      2.7  arithmetic   \n",
       "5  What is the difference between 2019 average ra...      0.2  arithmetic   \n",
       "\n",
       "                      derivation  \n",
       "1                            NaN  \n",
       "2                            NaN  \n",
       "3                    (2.9+2.9)/2  \n",
       "4                    (2.7+2.7)/2  \n",
       "5  [(2.9+2.9)/2] - [(2.7+2.7)/2]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_n = dataset[[\"question\", \"answer_type\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How much is the 2019 rate of inflation?</td>\n",
       "      <td>span</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How much is the 2018 rate of inflation?</td>\n",
       "      <td>span</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the 2019 average rate of inflation?</td>\n",
       "      <td>arithmetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the 2019 average rate of increase in s...</td>\n",
       "      <td>arithmetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is the difference between 2019 average ra...</td>\n",
       "      <td>arithmetic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question answer_type\n",
       "1            How much is the 2019 rate of inflation?        span\n",
       "2            How much is the 2018 rate of inflation?        span\n",
       "3        What is the 2019 average rate of inflation?  arithmetic\n",
       "4  What is the 2019 average rate of increase in s...  arithmetic\n",
       "5  What is the difference between 2019 average ra...  arithmetic"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_n.head()   #print the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = dataset_n[\"question\"]\n",
    "y = dataset_n[\"answer_type\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['What was the average settlements for 2017-2019?',\n",
       "       'What was the estimated useful life of Towers in years?',\n",
       "       'What is the average quarterly high sale price for 2019?',\n",
       "       'What does the table show?',\n",
       "       'What was the working capital in 2019?'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12630    arithmetic\n",
       "7307           span\n",
       "11472    arithmetic\n",
       "1573           span\n",
       "4538           span\n",
       "Name: answer_type, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "X_tfidf = vectorizer.fit_transform(X_train)\n",
    "random_state_seed = 42  # Set a specific seed for reproducibility\n",
    "model = LogisticRegression(random_state=random_state_seed)   #logistic regression model\n",
    "\n",
    "model.fit(X_tfidf, y_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'What was the change in the Total non-current trade and other payables in 2019 from 2018?' --> Prediction: arithmetic --> True Label: arithmetic\n",
      "Input: 'What is the percentage increase / (decrease) in Fuel Oils from 2018 to 2019?' --> Prediction: arithmetic --> True Label: arithmetic\n",
      "Input: 'What is the average hardware revenue from 2016 to 2018?' --> Prediction: arithmetic --> True Label: arithmetic\n",
      "Input: 'What is the percentage change in revenue generated from Partner C from 2018 to 2019?' --> Prediction: arithmetic --> True Label: arithmetic\n",
      "Input: 'As of March 29, 2019, What is Intangible assets expressed as a percentage of  Gross deferred tax assets?' --> Prediction: arithmetic --> True Label: arithmetic\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test_tfidf)\n",
    "i = 0\n",
    "for text, pred, true_label in zip(X_test, predictions, y_test):\n",
    "    print(f\"Input: '{text}' --> Prediction: {pred} --> True Label: {true_label}\")\n",
    "    i += 1\n",
    "    if i == 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt: How to save the vectorizer and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_model(data, file_name):\n",
    "#     os.makedirs(\"../artifacts/model\", exist_ok = True)\n",
    "#     try:\n",
    "#         file_path = f\"../artifacts/model/{str(file_name)}.pkl\"\n",
    "#         with open(file_path, \"wb\") as file:\n",
    "#             pickle.dump(data, file)\n",
    "#         print(f\"Data saved succesfully at: {file_path}\")\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Failed to save model due to: {str(e)}\")\n",
    "#         raise\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(data = vectorizer, file_name = \"vectorizer\")\n",
    "# save_model(data = model, file_name = \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt: How to measure the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic regression model: 0.9304224352147675\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Evaluate the model's performance\n",
    "accuracy1=  accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy of Logistic regression model:\", accuracy1)    #prints accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt: Other model approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'What was the change in the Total non-current trade and other payables in 2019 from 2018?' --> Prediction: arithmetic --> True Label: arithmetic\n",
      "Input: 'What is the percentage increase / (decrease) in Fuel Oils from 2018 to 2019?' --> Prediction: arithmetic --> True Label: arithmetic\n",
      "Input: 'What is the average hardware revenue from 2016 to 2018?' --> Prediction: arithmetic --> True Label: arithmetic\n",
      "Input: 'What is the percentage change in revenue generated from Partner C from 2018 to 2019?' --> Prediction: arithmetic --> True Label: arithmetic\n",
      "Input: 'As of March 29, 2019, What is Intangible assets expressed as a percentage of  Gross deferred tax assets?' --> Prediction: arithmetic --> True Label: arithmetic\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# Split data into texts (X) and labels (y)\n",
    "X = dataset_n[\"question\"]\n",
    "y = dataset_n[\"answer_type\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize and train the Naive Bayes model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "predictions = model.predict(X_test_tfidf)\n",
    "i = 0\n",
    "for text, pred, true_label in zip(X_test, predictions, y_test):\n",
    "    print(f\"Input: '{text}' --> Prediction: {pred} --> True Label: {true_label}\")\n",
    "    i += 1\n",
    "    if i == 5:\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive bayes: 0.8044378698224852\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance\n",
    "accuracy2=accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of Naive bayes:\", accuracy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'What was the change in the Total non-current trade and other payables in 2019 from 2018?' --> Prediction: arithmetic --> True Label: arithmetic\n",
      "Input: 'What is the percentage increase / (decrease) in Fuel Oils from 2018 to 2019?' --> Prediction: arithmetic --> True Label: arithmetic\n",
      "Input: 'What is the average hardware revenue from 2016 to 2018?' --> Prediction: arithmetic --> True Label: arithmetic\n",
      "Input: 'What is the percentage change in revenue generated from Partner C from 2018 to 2019?' --> Prediction: arithmetic --> True Label: arithmetic\n",
      "Input: 'As of March 29, 2019, What is Intangible assets expressed as a percentage of  Gross deferred tax assets?' --> Prediction: arithmetic --> True Label: arithmetic\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC  # Support Vector Classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split data into texts (X) and labels (y)\n",
    "X = dataset_n[\"question\"]\n",
    "y = dataset_n[\"answer_type\"]\n",
    "\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# # Initialize and train the SVM model\n",
    "# model = SVC(kernel='linear')  # Linear kernel for text classification\n",
    "# model.fit(X_train_tfidf, y_train)\n",
    "C_value = 1.0  # Regularization parameter (example value)\n",
    "gamma_value = 'scale'  # Gamma can be 'scale', 'auto', or a float value\n",
    "\n",
    "model = SVC(kernel='linear', C=C_value, gamma=gamma_value, random_state=42)  # Linear kernel for text classification\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "predictions = model.predict(X_test_tfidf)\n",
    "i = 0\n",
    "for text, pred, true_label in zip(X_test, predictions, y_test):\n",
    "    print(f\"Input: '{text}' --> Prediction: {pred} --> True Label: {true_label}\")\n",
    "    i += 1\n",
    "    if i == 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVC model: 0.9381656804733728\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance\n",
    "accuracy3=accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of SVC model:\", accuracy3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt: Random forest classifier with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'What was the change in the Total non-current trade and other payables in 2019 from 2018?' --> Prediction: arithmetic --> True Label: arithmetic\n",
      "Input: 'What is the percentage increase / (decrease) in Fuel Oils from 2018 to 2019?' --> Prediction: arithmetic --> True Label: arithmetic\n",
      "Input: 'What is the average hardware revenue from 2016 to 2018?' --> Prediction: arithmetic --> True Label: arithmetic\n",
      "Input: 'What is the percentage change in revenue generated from Partner C from 2018 to 2019?' --> Prediction: arithmetic --> True Label: arithmetic\n",
      "Input: 'As of March 29, 2019, What is Intangible assets expressed as a percentage of  Gross deferred tax assets?' --> Prediction: arithmetic --> True Label: arithmetic\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier  # Random Forest model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split data into texts (X) and labels (y)\n",
    "X = dataset_n[\"question\"]\n",
    "y = dataset_n[\"answer_type\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# # Initialize and train the Random Forest model\n",
    "# model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "min_samples_leaf_value = 2  # Example value for min_samples_leaf\n",
    "max_features_value = 'sqrt'  # Use square root of number of features\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    min_samples_leaf=min_samples_leaf_value,\n",
    "    max_features=max_features_value\n",
    ")\n",
    "\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "\n",
    "\n",
    "predictions = model.predict(X_test_tfidf)\n",
    "i = 0\n",
    "for text, pred, true_label in zip(X_test, predictions, y_test):\n",
    "    print(f\"Input: '{text}' --> Prediction: {pred} --> True Label: {true_label}\")\n",
    "    i += 1\n",
    "    if i == 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of randorm forest model: 0.9387573964497041\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance\n",
    "accuracy4=accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of randorm forest model:\",accuracy4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#NLP using tensorflow\\nglove->embedding 6b,100d\\n\\nembedding tf.keras.layers.Embeddings\\ntransformers \\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#NLP using tensorflow\n",
    "glove->embedding 6b,100d\n",
    "\n",
    "embedding tf.keras.layers.Embeddings\n",
    "transformers \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\esrav\\FinChatbot\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m247/247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7729 - loss: 0.4652 - val_accuracy: 0.9275 - val_loss: 0.2103\n",
      "Epoch 2/10\n",
      "\u001b[1m247/247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9418 - loss: 0.1695 - val_accuracy: 0.9308 - val_loss: 0.2056\n",
      "Epoch 3/10\n",
      "\u001b[1m247/247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9619 - loss: 0.1153 - val_accuracy: 0.9275 - val_loss: 0.2143\n",
      "Epoch 4/10\n",
      "\u001b[1m247/247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9739 - loss: 0.0846 - val_accuracy: 0.9234 - val_loss: 0.2630\n",
      "Epoch 5/10\n",
      "\u001b[1m247/247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9779 - loss: 0.0671 - val_accuracy: 0.9314 - val_loss: 0.2497\n",
      "Epoch 6/10\n",
      "\u001b[1m247/247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9842 - loss: 0.0465 - val_accuracy: 0.9222 - val_loss: 0.3078\n",
      "Epoch 7/10\n",
      "\u001b[1m247/247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9853 - loss: 0.0431 - val_accuracy: 0.9296 - val_loss: 0.2523\n",
      "Epoch 8/10\n",
      "\u001b[1m247/247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9874 - loss: 0.0367 - val_accuracy: 0.9320 - val_loss: 0.3104\n",
      "Epoch 9/10\n",
      "\u001b[1m247/247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9895 - loss: 0.0359 - val_accuracy: 0.9278 - val_loss: 0.2874\n",
      "Epoch 10/10\n",
      "\u001b[1m247/247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9895 - loss: 0.0343 - val_accuracy: 0.9269 - val_loss: 0.3150\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9315 - loss: 0.2931\n",
      "Test Accuracy: 0.9269\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Input: 'What was the change in the Total non-current trade and other payables in 2019 from 2018?' --> Prediction: 1 --> True Label: 1\n",
      "Input: 'What is the percentage increase / (decrease) in Fuel Oils from 2018 to 2019?' --> Prediction: 1 --> True Label: 1\n",
      "Input: 'What is the average hardware revenue from 2016 to 2018?' --> Prediction: 1 --> True Label: 1\n",
      "Input: 'What is the percentage change in revenue generated from Partner C from 2018 to 2019?' --> Prediction: 1 --> True Label: 1\n",
      "Input: 'As of March 29, 2019, What is Intangible assets expressed as a percentage of  Gross deferred tax assets?' --> Prediction: 1 --> True Label: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Split data into texts (X) and labels (y)\n",
    "X = dataset_n[\"question\"]\n",
    "y = dataset_n[\"answer_type\"]\n",
    "\n",
    "# Encode labels as integers (if not already encoded)\n",
    "y = pd.factorize(y)[0]  # Factorize the labels into integer form\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)  # Limit to 5000 features\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Convert to dense format (TensorFlow prefers dense input)\n",
    "X_train_tfidf = X_train_tfidf.toarray()\n",
    "X_test_tfidf = X_test_tfidf.toarray()\n",
    "\n",
    "# Convert labels to categorical (for multi-class classification)\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "\n",
    "# Build the NLP model in TensorFlow\n",
    "model = Sequential()\n",
    "\n",
    "# First layer (input layer)\n",
    "model.add(Dense(512, input_dim=X_train_tfidf.shape[1], activation='relu'))  # 512 neurons\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "\n",
    "# Second layer\n",
    "model.add(Dense(256, activation='relu'))  # 256 neurons\n",
    "model.add(Dropout(0.5))  # Dropout\n",
    "\n",
    "# Third layer\n",
    "model.add(Dense(128, activation='relu'))  # 128 neurons\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(y_train_cat.shape[1], activation='softmax'))  # Softmax for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_tfidf, y_train_cat, epochs=10, batch_size=32, validation_data=(X_test_tfidf, y_test_cat))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test_tfidf, y_test_cat)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Get the class with the highest probability\n",
    "\n",
    "# Print first 5 predictions\n",
    "i = 0\n",
    "for text, pred, true_label in zip(X_test, y_pred_classes, y_test):\n",
    "    print(f\"Input: '{text}' --> Prediction: {pred} --> True Label: {true_label}\")\n",
    "    i += 1\n",
    "    if i == 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\esrav\\FinChatbot\\.venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m247/247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7537 - loss: 0.5898 - val_accuracy: 0.9142 - val_loss: 0.2421\n",
      "Epoch 2/10\n",
      "\u001b[1m247/247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9328 - loss: 0.1984 - val_accuracy: 0.9311 - val_loss: 0.2070\n",
      "Epoch 3/10\n",
      "\u001b[1m247/247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9497 - loss: 0.1581 - val_accuracy: 0.9346 - val_loss: 0.2036\n",
      "Epoch 4/10\n",
      "\u001b[1m247/247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9590 - loss: 0.1340 - val_accuracy: 0.9266 - val_loss: 0.2098\n",
      "Epoch 5/10\n",
      "\u001b[1m247/247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9619 - loss: 0.1175 - val_accuracy: 0.9269 - val_loss: 0.2187\n",
      "Epoch 6/10\n",
      "\u001b[1m247/247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9654 - loss: 0.1091 - val_accuracy: 0.9281 - val_loss: 0.2256\n",
      "Epoch 7/10\n",
      "\u001b[1m247/247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9734 - loss: 0.0911 - val_accuracy: 0.9281 - val_loss: 0.2365\n",
      "Epoch 8/10\n",
      "\u001b[1m247/247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9682 - loss: 0.0920 - val_accuracy: 0.9269 - val_loss: 0.2459\n",
      "Epoch 9/10\n",
      "\u001b[1m247/247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9746 - loss: 0.0850 - val_accuracy: 0.9222 - val_loss: 0.2675\n",
      "Epoch 10/10\n",
      "\u001b[1m247/247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9721 - loss: 0.0858 - val_accuracy: 0.9260 - val_loss: 0.2768\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9321 - loss: 0.2561\n",
      "Test Accuracy: 0.9260\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Input: 'What was the change in the Total non-current trade and other payables in 2019 from 2018?' --> Prediction: 1 --> True Label: 1\n",
      "Input: 'What is the percentage increase / (decrease) in Fuel Oils from 2018 to 2019?' --> Prediction: 1 --> True Label: 1\n",
      "Input: 'What is the average hardware revenue from 2016 to 2018?' --> Prediction: 1 --> True Label: 1\n",
      "Input: 'What is the percentage change in revenue generated from Partner C from 2018 to 2019?' --> Prediction: 1 --> True Label: 1\n",
      "Input: 'As of March 29, 2019, What is Intangible assets expressed as a percentage of  Gross deferred tax assets?' --> Prediction: 1 --> True Label: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Split data into texts (X) and labels (y)\n",
    "X = dataset_n[\"question\"]\n",
    "y = dataset_n[\"answer_type\"]\n",
    "\n",
    "# Encode labels as integers (if not already encoded)\n",
    "y = pd.factorize(y)[0]  # Factorize the labels into integer form\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)  # Limit to 5000 features\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Convert to dense format (TensorFlow prefers dense input)\n",
    "X_train_tfidf = X_train_tfidf.toarray()\n",
    "X_test_tfidf = X_test_tfidf.toarray()\n",
    "\n",
    "# Convert labels to categorical (for multi-class classification)\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "\n",
    "# Reshape to 3D for LSTM\n",
    "# TF-IDF data is 2D (samples, features), we need to reshape it to 3D (samples, timesteps, features)\n",
    "# Let's use the number of features as the timestep for each word's representation\n",
    "\n",
    "timesteps = 1  # We treat each word as a single timestep (each word gets a single feature vector)\n",
    "X_train_3d = X_train_tfidf.reshape((X_train_tfidf.shape[0], timesteps, X_train_tfidf.shape[1]))\n",
    "X_test_3d = X_test_tfidf.reshape((X_test_tfidf.shape[0], timesteps, X_test_tfidf.shape[1]))\n",
    "\n",
    "# Build the NLP model with LSTM in TensorFlow\n",
    "model = Sequential()\n",
    "\n",
    "# First layer (input layer)\n",
    "model.add(LSTM(128, input_shape=(X_train_3d.shape[1], X_train_3d.shape[2]), return_sequences=True))  # LSTM with 128 units\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "\n",
    "# LSTM Layer (the main addition)\n",
    "model.add(LSTM(64))  # LSTM with 64 units\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(y_train_cat.shape[1], activation='softmax'))  # Softmax for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_3d, y_train_cat, epochs=10, batch_size=32, validation_data=(X_test_3d, y_test_cat))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test_3d, y_test_cat)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_3d)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Get the class with the highest probability\n",
    "\n",
    "# Print first 5 predictions\n",
    "i = 0\n",
    "for text, pred, true_label in zip(X_test, y_pred_classes, y_test):\n",
    "    print(f\"Input: '{text}' --> Prediction: {pred} --> True Label: {true_label}\")\n",
    "    i += 1\n",
    "    if i == 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\esrav\\FinChatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\esrav\\FinChatbot\\.venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1819, Accuracy: 0.9330\n",
      "Test Accuracy: 0.9544\n"
     ]
    }
   ],
   "source": [
    "#Before changes\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sample DataFrame (replace with actual dataset)\n",
    "dataset_n['answer_type'] = dataset_n['answer_type'].astype('category').cat.codes  # Label encoding\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataset_n['question'].tolist(), dataset_n['answer_type'].tolist(), test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Custom Dataset class\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=32):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx], truncation=True, padding='max_length',\n",
    "            max_length=self.max_length, return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = QADataset(X_train, y_train, tokenizer)\n",
    "test_dataset = QADataset(X_test, y_test, tokenizer)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, num_workers=2)\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "num_labels = len(set(y_train))\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 1\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss, correct = 0, 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.logits.argmax(dim=-1) == labels).sum().item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}, Accuracy: {correct/len(train_dataset):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "y_pred, y_true = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(**inputs)\n",
    "        y_pred.extend(outputs.logits.argmax(dim=-1).cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\esrav\\FinChatbot\\.venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Sample DataFrame (replace with actual dataset)\n",
    "# dataset_n['answer_type'] = dataset_n['answer_type'].astype('category').cat.codes  # Label encoding\n",
    "\n",
    "# # Split data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     dataset_n['question'].tolist(), dataset_n['answer_type'].tolist(), test_size=0.3, random_state=42\n",
    "# )\n",
    "\n",
    "# # Initialize tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Custom Dataset class\n",
    "# class QADataset(Dataset):\n",
    "#     def __init__(self, texts, labels, tokenizer, max_length=32):\n",
    "#         self.texts = texts\n",
    "#         self.labels = labels\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_length = max_length\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.texts)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         encoding = self.tokenizer(\n",
    "#             self.texts[idx], truncation=True, padding='max_length',\n",
    "#             max_length=self.max_length, return_tensors='pt'\n",
    "#         )\n",
    "#         item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "#         item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "#         return item\n",
    "\n",
    "# # Create datasets\n",
    "# train_dataset = QADataset(X_train, y_train, tokenizer)\n",
    "# test_dataset = QADataset(X_test, y_test, tokenizer)\n",
    "\n",
    "# # Number of workers for loading data (you can adjust this based on your system)\n",
    "# num_workers = 1  # You can change this based on the number of CPU cores or your system's capacity\n",
    "\n",
    "# # DataLoaders with num_workers\n",
    "# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=num_workers)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=16, num_workers=num_workers)\n",
    "\n",
    "# # Load pre-trained BERT model\n",
    "# num_labels = len(set(y_train))\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)\n",
    "\n",
    "# # Optimizer and loss function\n",
    "# optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# # Training loop\n",
    "# epochs = 1\n",
    "# model.train()\n",
    "# for epoch in range(epochs):\n",
    "#     total_loss, correct = 0, 0\n",
    "#     for batch in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "#         labels = batch['labels'].to(device)\n",
    "#         outputs = model(**inputs)\n",
    "#         loss = criterion(outputs.logits, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#         correct += (outputs.logits.argmax(dim=-1) == labels).sum().item()\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}, Accuracy: {correct/len(train_dataset):.4f}\")\n",
    "\n",
    "# # Evaluation\n",
    "# model.eval()\n",
    "# y_pred, y_true = [], []\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_loader:\n",
    "#         inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "#         labels = batch['labels'].to(device)\n",
    "#         outputs = model(**inputs)\n",
    "#         y_pred.extend(outputs.logits.argmax(dim=-1).cpu().numpy())\n",
    "#         y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "# accuracy = accuracy_score(y_true, y_pred)\n",
    "# print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the accuracy results as below:\n",
    "\n",
    "\n",
    "    Logistic regression with TF-IDF -> 93%\n",
    "    Naive Bayes with TF-IDF->  80%\n",
    "    SVC with TF-IDF->  93%\n",
    "    Random Forest Classifier with TF-IDF->  94%\n",
    "    NLP Model in tensor flow-> 92%\n",
    "    Added LSTM layer->92%\n",
    "    Prompt: make a new code using bert transformer using hugging face-> 95%\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, here we choose Bert transformer model as the best because the accuracy is high among all other models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
